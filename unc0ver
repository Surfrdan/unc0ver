#!/usr/bin/env python3

import argparse
from multiprocessing.dummy import Pool as ThreadPool
import requests
import os
import time
import sys
import urllib3
from bs4 import BeautifulSoup

from termcolor import colored

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

THREADS = 12

script_dir = os.path.dirname(os.path.realpath(__file__))
wordlist_file = script_dir + '/complete_wordlist.txt'

parser = argparse.ArgumentParser()
parser.add_argument('-u', type=str, help='Base URL e.g. https://example.com')
parser.add_argument('-w', type=str, help='Wordlist file e.g. wordlist.txt', nargs='?', const=wordlist_file)
parser.add_argument('-e', type=str, help='Optional file extension e.g. tpl')
parser.add_argument('-c', type=str, help='Case to convert input into extension e.g. upper')
parser.add_argument('-r', type=str, help='Range of numbers to generate numeric filenames e.g. 1:300')
parser.add_argument('-f', action='store_true', help='Follow Redirects off by default')
parser.add_argument('-l', type=str, help='Rate Limit in requests per minute')
parser.add_argument('-n', type=str, help='Note field preference, defaults to Server header or Location on a 3xx e.g. title will return contents of <title> HTML tag instead. USefuo for sites who return 200 for everything')
args = parser.parse_args()

pool = ThreadPool(THREADS)

def make_request(url):
    headers = {
        'User-Agent': 'unc0ver HTTP Reconnaissance 0.1'
    }
    if args.e:
        url = '{0}.{1}'.format(url,args.e)
 
    if args.l:
        time.sleep(int(args.l))

    follow = False    
    if args.f:
        follow = True
   
    note = ''
    try:
        r = requests.get(url, verify=False, allow_redirects=follow, headers=headers)
        status_code = r.status_code

        if args.n:
            if args.n == 'title':
                soup = BeautifulSoup(r.text, "html.parser")
                title = soup.find('title')
                if title:
                    note = title.text
            if args.n == 'h1':
                soup = BeautifulSoup(r.text, "html.parser")
                title = soup.find('h1')
                if title:
                    note = title.text

    except requests.exceptions.ConnectionError as e:
        status_code = 000
    except requests.exceptions.RequestException as e:
        status_code = 000
        
    if status_code in range(200,226,1):
        status_code = colored(status_code, 'green')
        if ('server' in r.headers):
            if not args.n:
                note = r.headers['server']

    if status_code in [301, 302, 303, 304, 305, 306, 307, 308]:
        if ('location' in r.headers):
            status_code = colored(status_code, 'yellow')
            if not args.n:
                note = r.headers['Location']

    if status_code in range(500,511,1):
        status_code = colored(status_code, 'red')
        if ('server' in r.headers):
            if not args.n:
                note = r.headers['server']

    if status_code in [400,401,402,403,404]:
        status_code = colored(status_code, 'red')
        if ('server' in r.headers):
            if not args.n:
                note = r.headers['server']

    print ('{0} {1} {2}'.format(status_code, url, note).strip('\r\n'))
    if status_code is not 000:
        return r
    else:
        return False

if args.u:
    base_url = args.u
else:
    print('Please supply a base URL with -u')
    sys.exit(0)
try: 
    args.w
    wordlist_file = args.w
    wordlist = open(wordlist_file, encoding = "ISO-8859-1")
    words = wordlist.read().splitlines()
except NameError:
    pass

if args.r:
    number_range = args.r.split(':')
    words = range(int(number_range[0]),int(number_range[1]))

urls = list()

for word in words:
    word = str(word.strip())

    if args.c == 'upper':
        word = word.upper()
    if args.c == 'lower':
        word = word.lower()

    if not word.startswith('/'):
        word = '/{0}'.format(word)
    urls.append('{0}{1}'.format(base_url,word))

new_results = pool.map(make_request, urls)
pool.close()
pool.join()
