#!/usr/bin/env python3

import argparse
from multiprocessing.dummy import Pool as ThreadPool
import requests
import os
import datetime
import random
import time
import sys
import urllib3
from bs4 import BeautifulSoup

from termcolor import colored

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

request_count = 0

THREADS = 120

script_dir = os.path.dirname(os.path.realpath(__file__))
wordlist_file = script_dir + '/wordlists/complete_wordlist.txt'

parser = argparse.ArgumentParser()
parser.add_argument('-w', type=str, help='Wordlist file e.g. wordlist.txt', nargs='?', const=wordlist_file)
parser.add_argument('-u', type=str, help='Base URL e.g. https://example.com')
parser.add_argument('-b', action='store_true', help='Include body in output')
parser.add_argument('-e', type=str, help='Optional file extension e.g. tpl')
parser.add_argument('-p', type=str, help='Optional prefix e.g. "?" or "&" useful for querystrings')
parser.add_argument('-c', type=str, help='Case to convert input into extension e.g. upper')
parser.add_argument('-r', type=str, help='Range of numbers to generate numeric filenames e.g. 1:300')
parser.add_argument('-f', action='store_true', help='Follow Redirects off by default')
parser.add_argument('-l', type=str, help='Rate Limit in requests per minute')
parser.add_argument('-n', type=str, help='Note field preference, defaults to Server header or Location on a 3xx options are title,h1,h2,meta')
args = parser.parse_args()

pool = ThreadPool(THREADS)

def timestamp():
    return datetime.datetime.now().strftime("%X")

def get_random_user_agent():
    user_agents = [
        'Mozilla/5.0 (compatible, MSIE 11, Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',
        'Mozilla/4.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/5.0)',
        'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201',
        'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9a3pre) Gecko/20070330',
        'Mozilla/5.0 (X11; ; Linux i686; rv:1.9.2.20) Gecko/20110805',
        'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1',
        'Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A',
        'Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5355d Safari/8536.25',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/537.13+ (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2'
    ]
    return random.choice(user_agents)

def make_request(url):
    global request_count
    global file_count
    request_count += 1
    user_agent = get_random_user_agent()
    headers = {
        'User-Agent': user_agent 
    }
    if args.e:
        url = '{0}{1}'.format(url,args.e)
 
    if args.l:
        time.sleep(int(args.l))

    follow = False    
    if args.f:
        follow = True
   
    note = ''
    r = None
    try:
        r = requests.get(url, verify=False, allow_redirects=follow, headers=headers)
        status_code = r.status_code
        size = len(r.text)
        if args.n:
            soup = BeautifulSoup(r.text, "html.parser")
            if args.n == 'title':
                title = soup.find('title')
                if title:
                    note = title.text
            if args.n == 'h1':
                title = soup.find('h1')
                if title:
                    note = title.text
            if args.n == 'meta':
                soup = BeautifulSoup(r.text, "html.parser")
                title = soup.find("div", attrs={"id": "app"})
                if title:
                    note = title.text

    except Exception as e:
        status_code = 0

    if status_code in [0]:
        status_code = colored('000', 'red')
        size = '000'

    if status_code in range(200,226,1):
        status_code = colored(status_code, 'green')
        if ('server' in r.headers):
            if not args.n:
                note = r.headers['server']

    if status_code in [301, 302, 303, 304, 305, 306, 307, 308]:
        if ('location' in r.headers):
            status_code = colored(status_code, 'yellow')
            if not args.n:
                note = r.headers['Location']

    if status_code in range(500,511,1):
        status_code = colored(status_code, 'red')
        if ('server' in r.headers):
            if not args.n:
                note = r.headers['server']

    if status_code in [400,401,402,403,404]:
        status_code = colored(status_code, 'red')
        if ('server' in r.headers):
            if not args.n:
                note = r.headers['server']
    counter = '[{0}/{1}]'.format(request_count,file_count)
    content_type = 'none'
    if 'Content-Type' in r.headers:
    	content_type = r.headers['Content-Type']

    print ('{0} {1} {2}B {3} {4} {5} {6}'.format(status_code, timestamp(), size, counter, content_type, url, note).strip('\r\n'))

    if args.b:
        try:
            print (r.content)
        except Exception:
            pass

def check_security_txt(base_url):
    try:
        security_txt = None
        parsed_url = urllib3.util.parse_url(base_url)
        url = 'http://'
        if parsed_url[1]:
            url = '{}://'.format(parsed_url[1])
        url += parsed_url[2]
        if parsed_url[3]:
            url += ':{}'.format(parsed_url[3])	
        url = '{0}/.well-known/security.txt'.format(url)
        r = requests.get(url=url, verify=False, timeout=3)
        if r.text and 'text/plain' == r.headers['Content-Type']:
            security_txt = r.text
        else:
            url = '{0}/security.txt'.format(url)
            r = requests.get(url=url, verify=False, timeout=3)
            if r.text and 'text/plain' == r.headers['Content-Type']:
                security_txt = r.text
        if security_txt:
            print('security.txt file found for domain')
            print(security_txt)
            input("Press Enter to continue...")
    except Exception:
        pass;
      
banner = """ 
                       ___
     _   _ _ __   ___ / _ \__   _____ _ __
    | | | | '_ \ / __| | | \ \ / / _ \ '__|
    | |_| | | | | (__| |_| |\ V /  __/ |
     \__,_|_| |_|\___|\___/  \_/ \___|_|
       An HTTP Recon Tool - By Surfrdan
"""
print(colored(banner, 'magenta'))

if args.u:
    base_url = args.u
    #check_security_txt(base_url)
else:
    print('Please supply a base URL with -u')
    sys.exit(0)
try: 
    if args.r:
        number_range = args.r.split(':')
        words = range(int(number_range[0]),int(number_range[1]))
    else:
        wordlist_file = args.w
        wordlist = open(wordlist_file, encoding = "ISO-8859-1")
        words = wordlist.read().splitlines()
except NameError:
    pass

urls = list()

file_count= 0
for word in words:
    file_count += 1
    word = str(word).strip()

    if args.c == 'upper':
        word = word.upper()
    if args.c == 'lower':
        word = word.lower()
    
    if args.p:
        word = '{0}{1}'.format(args.p, word)
    
    if word.startswith('/'):
        word = word[1:]
    urls.append('{0}{1}'.format(base_url,word))

new_results = pool.map_async(make_request, urls)
pool.close()
pool.join()
