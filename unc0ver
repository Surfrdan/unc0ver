#!/usr/bin/env python3

import argparse
from multiprocessing.dummy import Pool as ThreadPool
import requests
import os
import datetime
import random
import time
import urllib3
from bs4 import BeautifulSoup
from termcolor import colored

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
script_dir = os.path.dirname(os.path.realpath(__file__))
status_tally = dict() 

def parse_args():
    arg_w_help = 'Wordlist file e.g. wordlist.txt'
    arg_s_help = 'Timeout in seconds'
    arg_t_help = 'Number of threads to use. Defaults to 120 which can be too much on slow machines or connections'
    arg_u_help = 'Base URL e.g. https://example.com'
    arg_b_help = 'Include body in output'
    arg_e_help = 'Optional file extension e.g. .tpl'
    arg_p_help = 'Optional prefix e.g. "?" or "&" useful for querystring or directory fuzzing'
    arg_c_help = 'Case to convert input into extension e.g. upper'
    arg_r_help = 'Case to convert input into extension e.g. upper'
    arg_i_help = 'list of HTTP response codes to ignore. Defaults to 403,404. Set this to blank to include 404s or 403s or specify a list of comma seperated codes to ignore'
    arg_f_help = 'Follow Redirects off by default'
    arg_l_help = 'Rate Limit in requests per minute'
    arg_n_help = 'Note field preference, defaults to Server header or Location on a 3xx options are title,h1,h2,meta'

    parser = argparse.ArgumentParser()
    parser.add_argument('-w', '--wordlist', type=str, help=arg_w_help, nargs='?', default=script_dir + '/wordlists/complete_wordlist.txt')
    parser.add_argument('-s', '--timeout', type=int, help=arg_s_help, default=10)
    parser.add_argument('-t', '--threads', type=int, help=arg_t_help, default=120)
    parser.add_argument('-u', '--base_url', type=str, help=arg_u_help, required=True)
    parser.add_argument('-b', '--body', action='store_true', help=arg_b_help, default=False)
    parser.add_argument('-e', '--extension', type=str, help=arg_e_help, default=False)
    parser.add_argument('-p', '--prefix', type=str, help=arg_p_help, default=False)
    parser.add_argument('-c', '--case', type=str, help=arg_c_help, default=False)
    parser.add_argument('-r', '--range', type=str, help=arg_r_help, default=False)
    parser.add_argument('-i', '--ignore', type=str, help=arg_i_help, default=[403,404])
    parser.add_argument('-f', '--follow', action='store_true', help=arg_f_help, default=False)
    parser.add_argument('-l', '--ratelimit', type=int, help=arg_l_help, default=0)
    parser.add_argument('-n', '--note', type=str, help=arg_n_help, default=False)
    return parser.parse_args()

def print_banner():
    banner = """ 
                           ___
         _   _ _ __   ___ / _ \__   _____ _ __
        | | | | '_ \ / __| | | \ \ / / _ \ '__|
        | |_| | | | | (__| |_| |\ V /  __/ |
         \__,_|_| |_|\___|\___/  \_/ \___|_|
           An HTTP Recon Tool - By Surfrdan
    """
    print(colored(banner, 'magenta'))

def timestamp():
    return datetime.datetime.now().strftime("%X")

def get_random_user_agent():
    user_agents = [
        'Mozilla/5.0 (compatible, MSIE 11, Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',
        'Mozilla/4.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/5.0)',
        'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201',
        'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9a3pre) Gecko/20070330',
        'Mozilla/5.0 (X11; ; Linux i686; rv:1.9.2.20) Gecko/20110805',
        'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1',
        'Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A',
        'Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5355d Safari/8536.25',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/537.13+ (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2'
    ]
    return random.choice(user_agents)

def tally_status(status_code):
    global status_tally
    status_code = str(status_code)
    status_tally[status_code] += 1

def get_status_tally():
    for code, count in status_tally.items():
        print("{0} {1}".format(code, count))

def make_request(url):
    global request_count
    request_count += 1
    user_agent = get_random_user_agent()
    headers = {
        'User-Agent': user_agent 
    }
    if args.extension:
        url = '{0}{1}'.format(url,args.extension)

    if args.ratelimit:
        time.sleep(args.ratelimit)

    note = ''
    r = None
    try:
        r = requests.get(url, verify=False, allow_redirects=args.follow, headers=headers, timeout=args.timeout)
        status_code = r.status_code
        if (status_code in args.ignore):
            return
        size = len(r.text)
        if args.note:
            soup = BeautifulSoup(r.text, "html.parser")
            if args.note == 'title':
                title = soup.find('title')
                if title:
                    note = title.text
            if args.note == 'h1':
                title = soup.find('h1')
                if title:
                    note = title.text
            if args.note == 'meta':
                soup = BeautifulSoup(r.text, "html.parser")
                title = soup.find("div", attrs={"id": "app"})
                if title:
                    note = title.text
    except (ConnectionError, ProxyError, SSLError, Timeout) as e:
        status_code = 0

    #tally_status(status_code)
    if status_code in [0]:
        status_code = colored('000', 'red')
        size = '000'

    if status_code in range(200,226,1):
        status_code = colored(status_code, 'green')
        if ('server' in r.headers):
            if not args.note:
                note = r.headers['server']

    if status_code in [301, 302, 303, 304, 305, 306, 307, 308]:
        if ('location' in r.headers):
            status_code = colored(status_code, 'yellow')
            if not args.note:
                note = r.headers['Location']

    if status_code in range(500,511,1):
        status_code = colored(status_code, 'red')
        if ('server' in r.headers):
            if not args.note:
                note = r.headers['server']

    if status_code in [400,401,402,403,404]:
        status_code = colored(status_code, 'red')
        if ('server' in r.headers):
            if not args.note:
                note = r.headers['server']
    counter = '[{0}/{1}]'.format(request_count,file_count)
    content_type = 'none'
    if 'Content-Type' in r.headers:
        content_type = r.headers['Content-Type']

    response_time = None
    if (r.elapsed.total_seconds() > args.timeout - 2):
        response_time = colored(str(int(r.elapsed.total_seconds())), 'red')
    else:
        response_time = colored(str(int(r.elapsed.total_seconds())), 'green')

    print ('{0} {1} {2}s {3}B {4} {5} {6} {7}'.format(status_code, timestamp(), response_time, size, counter, content_type, url, note).strip('\r\n'))

    if args.body:
        try:
            print (r.content)
        except Exception:
            pass

def check_security_txt():
    try:
        security_txt = None
        valid_security_txt = False
        parsed_url = urllib3.util.parse_url(args.base_url)
        url = 'http://'
        if parsed_url[1]:
            url = '{}://'.format(parsed_url[1])
        url += parsed_url[2]
        if parsed_url[3]:
            url += ':{}'.format(parsed_url[3])	
        url = '{0}/.well-known/security.txt'.format(url)
        r = requests.get(url=url, verify=False, timeout=args.timeout)
        if r.text:
            security_txt = r.text
        else:
            url = '{0}/security.txt'.format(url)
            r = requests.get(url=url, verify=False, timeout=args.timeout)
            if r.text:
                security_txt = r.text
        if security_txt:
            for line in security_txt.splitlines():
                if line.startswith('Contact:'):
                    valid_security_txt = True
            if security_txt and valid_security_txt:
                print('security.txt file found for domain')
                print(security_txt)
                input("Press Enter to continue...")
        else:
            pass
    except Exception:
        pass


print_banner()
args = parse_args()
check_security_txt()

try:
    if args.range:
        number_range = args.range.split(':')
        words = range(int(number_range[0]),int(number_range[1]))
    else:
        wordlist = open(args.wordlist, encoding = "ISO-8859-1")
        words = wordlist.read().splitlines()
except NameError:
    pass

urls = list()
file_count = 0

for word in words:
    file_count += 1
    word = str(word).strip()

    if args.case == 'upper':
        word = word.upper()
    if args.case == 'lower':
        word = word.lower()
    
    if args.prefix:
        word = '{0}{1}'.format(args.prefix, word)
    
    if word.startswith('/'):
        word = word[1:]
    urls.append('{0}{1}'.format(args.base_url,word))

request_count = 0
pool = ThreadPool(args.threads)
new_results = pool.map_async(make_request, urls)
pool.close()
pool.join()
get_status_tally()
