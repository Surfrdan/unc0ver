#!/usr/bin/env python3

import argparse
from multiprocessing.dummy import Pool as ThreadPool
import requests
import os
import datetime
import time
import sys
import urllib3
from bs4 import BeautifulSoup

from termcolor import colored

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

request_count = 0

THREADS = 120

script_dir = os.path.dirname(os.path.realpath(__file__))
wordlist_file = script_dir + '/complete_wordlist.txt'

parser = argparse.ArgumentParser()
parser.add_argument('-w', type=str, help='Wordlist file e.g. wordlist.txt', nargs='?', const=wordlist_file)
parser.add_argument('-u', type=str, help='Base URL e.g. https://example.com')
parser.add_argument('-b', action='store_true', help='Include body in output')
parser.add_argument('-e', type=str, help='Optional file extension e.g. tpl')
parser.add_argument('-p', type=str, help='Optional prefix e.g. "?" or "&" useful for querystrings')
parser.add_argument('-c', type=str, help='Case to convert input into extension e.g. upper')
parser.add_argument('-r', type=str, help='Range of numbers to generate numeric filenames e.g. 1:300')
parser.add_argument('-f', action='store_true', help='Follow Redirects off by default')
parser.add_argument('-l', type=str, help='Rate Limit in requests per minute')
parser.add_argument('-n', type=str, help='Note field preference, defaults to Server header or Location on a 3xx e.g. title will return contents of <title> HTML tag instead. USefuo for sites who return 200 for everything')
args = parser.parse_args()

pool = ThreadPool(THREADS)

def timestamp():
    return datetime.datetime.now().strftime("%X")
def make_request(url):
    global request_count
    global file_count
    request_count += 1
    headers = {
	#'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'
        #'User-Agent': 'unc0ver HTTP Reconnaissance 0.3'
        'User-Agent': 'Something Else'
    }
    if args.e:
        url = '{0}{1}'.format(url,args.e)
 
    if args.l:
        time.sleep(int(args.l))

    follow = False    
    if args.f:
        follow = True
   
    note = ''
    r = None
    try:
        r = requests.get(url, verify=False, allow_redirects=follow, headers=headers)
        status_code = r.status_code
        size = len(r.text)
        if args.n:
            soup = BeautifulSoup(r.text, "html.parser")
            if args.n == 'title':
                title = soup.find('title')
                if title:
                    note = title.text
            if args.n == 'h1':
                title = soup.find('h1')
                if title:
                    note = title.text
    except Exception as e:
        status_code = 0

    if status_code in [0]:
        status_code = colored('000', 'red')
        size = '000'

    if status_code in range(200,226,1):
        status_code = colored(status_code, 'green')
        if ('server' in r.headers):
            if not args.n:
                note = r.headers['server']

    if status_code in [301, 302, 303, 304, 305, 306, 307, 308]:
        if ('location' in r.headers):
            status_code = colored(status_code, 'yellow')
            if not args.n:
                note = r.headers['Location']

    if status_code in range(500,511,1):
        status_code = colored(status_code, 'red')
        if ('server' in r.headers):
            if not args.n:
                note = r.headers['server']

    if status_code in [400,401,402,403,404]:
        status_code = colored(status_code, 'red')
        if ('server' in r.headers):
            if not args.n:
                note = r.headers['server']
    counter = '[{0}/{1}]'.format(request_count,file_count)
    content_type = 'none'
    if 'Content=type' in r.headers:
    	content_type = r.headers['Content-Type']

    print ('{0} {1} {2}B {3} {4} {5} {6}'.format(status_code, timestamp(), size, counter, content_type, url, note).strip('\r\n'))

    if args.b:
        try:
            print (r.content)
        except Exception:
            pass

def check_security_txt(base_url):
	parsed_url = urllib3.util.parse_url(base_url)
	url = 'http://'
	if parsed_url[1]:
		url = '{}://'.format(parsed_url[1])
	url += parsed_url[2]
	if parsed_url[3]:
		url += ':{}'.format(parsed_url[3])	
	url = '{0}/.well-known/security.txt'.format(url)
	r = requests.get(url=url, verify=False)
	if r.text:
		print('security.txt file found for domain')
		print(r.text)

banner = """ 
                       ___
     _   _ _ __   ___ / _ \__   _____ _ __
    | | | | '_ \ / __| | | \ \ / / _ \ '__|
    | |_| | | | | (__| |_| |\ V /  __/ |
     \__,_|_| |_|\___|\___/  \_/ \___|_|
       An HTTP Recon Tool - By Surfrdan
"""
print(colored(banner, 'magenta'))

if args.u:
    base_url = args.u
    check_security_txt(base_url)
else:
    print('Please supply a base URL with -u')
    sys.exit(0)
try: 
    if args.r:
        number_range = args.r.split(':')
        words = range(int(number_range[0]),int(number_range[1]))
    else:
        wordlist_file = args.w
        wordlist = open(wordlist_file, encoding = "ISO-8859-1")
        words = wordlist.read().splitlines()
except NameError:
    pass

urls = list()

file_count= 0
for word in words:
    file_count += 1
    word = str(word).strip()

    if args.c == 'upper':
        word = word.upper()
    if args.c == 'lower':
        word = word.lower()
    
    if args.p:
        word = '{0}{1}'.format(args.p, word)
    
    # strip prefix slashes
    if word.startswith('/'):
        word = word[1:]
    urls.append('{0}{1}'.format(base_url,word))

new_results = pool.map_async(make_request, urls)
pool.close()
pool.join()
